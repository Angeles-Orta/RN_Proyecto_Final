# üé¨ An√°lisis de Sentimientos con LSTM - IMDB Reviews

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Proyecto final del curso de **Redes Neuronales** - Facultad de Ciencias, UNAM

**Autores:**
- Orta Castillo Maria de los Angeles - 319074253
- Solano Ju√°rez Sebasti√°n - 319254639

**Fecha de entrega:** 7 de diciembre del 2025

---

## üìã Tabla de Contenidos

- [Descripci√≥n](#-descripci√≥n)
- [Resultados](#-resultados)
- [Arquitectura](#-arquitectura)
- [Instalaci√≥n](#-instalaci√≥n)
- [Uso](#-uso)
- [Dataset](#-dataset)
- [Entrenamiento](#-entrenamiento)
- [Requisitos del Proyecto](#-requisitos-del-proyecto)

---

## üéØ Descripci√≥n

Este proyecto implementa un modelo de **Red Neuronal Recurrente (LSTM)** para clasificar autom√°ticamente rese√±as de pel√≠culas como **positivas** o **negativas**. El modelo fue entrenado usando el dataset IMDB Movie Reviews con PyTorch.

### ¬øPor qu√© LSTM?

Las redes LSTM (Long Short-Term Memory) son especialmente efectivas para:
- ‚úÖ Capturar dependencias a largo plazo en secuencias de texto
- ‚úÖ Manejar el problema del desvanecimiento del gradiente
- ‚úÖ Procesar secuencias de longitud variable
- ‚úÖ Balance entre rendimiento y costo computacional

---

## üìä Resultados

El modelo alcanz√≥ los siguientes resultados en el conjunto de prueba (25,000 rese√±as):

| M√©trica | Valor |
|---------|-------|
| **Accuracy** | 70.65% |
| **Precision** | 70.82% |
| **Recall** | 70.24% |
| **F1-Score** | 70.53% |
| **AUC-ROC** | 73.93% |

### Matriz de Confusi√≥n

```
                Predicci√≥n
                Neg     Pos
Real    Neg   [8,882  3,618]
        Pos   [3,720  8,780]
```

### Curvas de Aprendizaje

![Training History](training_history.png)

**Observaciones:**
- El modelo se entren√≥ durante 11 √©pocas (early stopping)
- Mejor modelo obtenido en la √©poca 8
- Train Accuracy: 75.65% | Validation Accuracy: 71.00%

---

## üèóÔ∏è Arquitectura

El modelo utiliza una arquitectura LSTM de dos capas con regularizaci√≥n:

```
Input (Secuencia de palabras)
    ‚Üì
Embedding Layer (vocab_size: 10,000 ‚Üí dim: 128)
    ‚Üì
LSTM Layer 1 (128 ‚Üí 64 unidades)
    ‚Üì
Dropout (0.5)
    ‚Üì
LSTM Layer 2 (64 ‚Üí 32 unidades)
    ‚Üì
Dropout (0.5)
    ‚Üì
Fully Connected (32 ‚Üí 1)
    ‚Üì
Sigmoid Activation
    ‚Üì
Output (Probabilidad: 0-1)
```

**Par√°metros totales:** 1,342,241

### Hiperpar√°metros

| Hiperpar√°metro | Valor |
|----------------|-------|
| Vocabulario | 10,000 palabras |
| Embedding Dim | 128 |
| LSTM Hidden (capa 1) | 64 |
| LSTM Hidden (capa 2) | 32 |
| Dropout Rate | 0.5 |
| Batch Size | 64 |
| Learning Rate | 0.001 |
| Max Epochs | 15 |
| Early Stopping Patience | 3 |
| Max Sequence Length | 256 tokens |

---

## üîß Instalaci√≥n

### Requisitos

- Python 3.8+
- PyTorch
- TensorFlow (solo para cargar dataset IMDB)
- NumPy, Pandas, Scikit-learn

### Instalaci√≥n r√°pida

```bash
# Clonar el repositorio
git clone https://github.com/tu-usuario/lstm-sentiment-analysis.git
cd lstm-sentiment-analysis

# Instalar dependencias
pip install -r requirements.txt
```

### requirements.txt

```txt
torch>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
tensorflow>=2.13.0
matplotlib>=3.7.0
tqdm>=4.65.0
pandas>=2.0.0
```

---

## üöÄ Uso

### Opci√≥n 1: Inferencia con modelo pre-entrenado (Recomendado)

**No necesitas entrenar desde cero.** Descarga los pesos pre-entrenados y usa el script de inferencia.


#### 1. Ejecutar inferencia

```bash
python inference.py
```

#### 2. Salida esperada

```
=== CARGANDO MODELO PRE-ENTRENADO ===

Usando dispositivo: cpu
Cargando pesos desde: sentiment_model_weights.pth

Configuraci√≥n del modelo:
  vocab_size: 10000
  embedding_dim: 128
  hidden_dim: 64
  dropout_rate: 0.5
  max_length: 256

Par√°metros totales: 1,342,241

‚úì Modelo cargado exitosamente

======================================================================
DEMOSTRACI√ìN DE INFERENCIA
======================================================================

Rese√±a 1:
  Texto: This movie was absolutely fantastic! Great acting...
  Sentimiento: Positivo
  Probabilidad: 0.9234 (92.34%)
  Confianza: Alta

Rese√±a 2:
  Texto: Terrible waste of time. Boring and predictable...
  Sentimiento: Negativo
  Probabilidad: 0.0876 (8.76%)
  Confianza: Alta

...

¬øDeseas probar el modelo con tus propias rese√±as? (s/n):
```

#### 4. Modo interactivo

El script incluye un modo interactivo donde puedes escribir tus propias rese√±as:

```
Rese√±a: This film is a masterpiece!

  ‚Üí Sentimiento: Positivo
  ‚Üí Probabilidad: 0.9456 (94.56%)
  ‚Üí Confianza: Alta
```

### Opci√≥n 2: Uso program√°tico

Integra el modelo en tus propios scripts:

```python
from inference import load_trained_model, predict_sentiment

# Cargar modelo
model, word_index, config = load_trained_model(
    weights_path='sentiment_model_weights.pth',
    word_index_path='word_index.pkl'
)

# Predecir sentimiento
review = "This movie was amazing and exceeded all my expectations!"
sentiment, probability, confidence = predict_sentiment(
    model, review, word_index, 
    max_length=config['max_length']
)

print(f"Sentimiento: {sentiment}")
print(f"Probabilidad: {probability:.2%}")
print(f"Confianza: {confidence}")
```

**Salida:**
```
Sentimiento: Positivo
Probabilidad: 94.32%
Confianza: Alta
```

### Opci√≥n 3: Entrenar desde cero

Si deseas entrenar el modelo desde cero:

```bash
python rn_proy_final.py
```

‚è±Ô∏è **Tiempo estimado:**
- CPU: 2-3 horas
- GPU: 20-30 minutos

**Archivos generados:**
- `sentiment_model_weights.pth` - Pesos del mejor modelo
- `word_index.pkl` - Diccionario de vocabulario
- `best_model_checkpoint.pth` - Checkpoint completo
- `training_history.png` - Gr√°ficas de entrenamiento

---

## üóÇÔ∏è Dataset

### IMDB Movie Reviews

El dataset utilizado es uno de los m√°s populares en NLP:

- **Total:** 50,000 rese√±as de pel√≠culas
- **Clases:** Binarias (Positivo / Negativo)
- **Divisi√≥n:**
  - Train: 20,000 muestras (80%)
  - Validation: 5,000 muestras (20%)
  - Test: 25,000 muestras

### Preprocesamiento

1. **Tokenizaci√≥n:** Cada palabra se convierte en un √≠ndice num√©rico
2. **Vocabulario:** Se utilizan las 10,000 palabras m√°s frecuentes
3. **Padding:** Secuencias normalizadas a 256 tokens
   - Secuencias cortas: Se a√±aden ceros al final
   - Secuencias largas: Se truncan

**Ejemplo de secuencia:**
```python
Original: "This movie was great!"
Tokenizada: [12, 23, 45, 789, 2]
Padded: [12, 23, 45, 789, 2, 0, 0, 0, ..., 0]  # Hasta 256
```

**Link del dataset para descargar:**
Puedes descargar el dataset desde [IMDB Dataset of 50K Movie Reviews en Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).

---

## üèãÔ∏è Entrenamiento

### Configuraci√≥n

- **Optimizador:** Adam (lr=0.001)
- **Funci√≥n de p√©rdida:** Binary Cross Entropy
- **Early Stopping:** Paciencia de 3 √©pocas
- **Regularizaci√≥n:** Dropout (0.5)

### Proceso

El entrenamiento se realiz√≥ en **Google Colab** con los siguientes pasos:

```python
# 1. Cargar y preprocesar datos
X_train, X_val, X_test, y_train, y_val, y_test = load_imdb_data()

# 2. Crear modelo
model = SentimentLSTM(
    vocab_size=10000,
    embedding_dim=128,
    hidden_dim=64,
    dropout_rate=0.5
)

# 3. Entrenar con early stopping
# Mejor modelo: √âpoca 8
# Early stopping: √âpoca 11
```

### Evoluci√≥n del entrenamiento

| √âpoca | Train Loss | Train Acc | Val Loss | Val Acc |
|-------|-----------|-----------|----------|---------|
| 1 | 0.6927 | 51.01% | 0.6910 | 51.86% |
| 4 | 0.6195 | 65.03% | 0.6332 | 68.20% |
| 8 | 0.5374 | 75.65% | 0.6041 | **71.00%** ‚≠ê |
| 11 | 0.4951 | 78.11% | 0.6261 | 71.54% |

 **Mejor modelo guardado en √©poca 8**


---

##  Requisitos del Proyecto

Este proyecto cumple con todos los requisitos del curso:

### ‚úì Requisitos Obligatorios

- [x] **Dataset de datos no estructurados:** Texto (rese√±as IMDB)
- [x] **Capa compleja:** 2 capas LSTM (recurrentes)
- [x] **Regularizaci√≥n:** Dropout (50%)
- [x] **Hiperpar√°metros documentados:** Tabla completa con justificaciones
- [x] **M√©tricas de sklearn:** Accuracy, Precision, Recall, F1-Score, AUC-ROC
- [x] **Framework:** PyTorch
- [x] **C√≥digo documentado:** Comentarios y docstrings completos

### ‚úì Requisitos de Entrega

- [x] **Pesos del modelo disponibles:** Para inferencia sin reentrenamiento
- [x] **Script de inferencia funcional:** `inference.py`
- [x] **Enlace de descarga:** Google Drive con archivos necesarios
- [x] **Documentaci√≥n de uso:** README completo

---

## An√°lisis de Resultados

### Fortalezas del Modelo

‚úÖ **Balance entre clases:** FP ‚âà FN (sin sesgo)
‚úÖ **Generalizaci√≥n adecuada:** No overfitting excesivo
‚úÖ **Eficiencia:** Modelo ligero (1.3M par√°metros)
‚úÖ **Tiempo de inferencia:** < 1 segundo por rese√±a

### Limitaciones

‚ö†Ô∏è **Accuracy 70.65%:** Modelos Transformer (BERT) superan 90%
‚ö†Ô∏è **Sarcasmo e iron√≠a:** Dif√≠ciles de detectar con LSTM
‚ö†Ô∏è **Dependencias largas:** Algunos contextos muy extensos se pierden

### Comparaci√≥n con Estado del Arte

| Modelo | Accuracy | Par√°metros | Tiempo Inferencia |
|--------|----------|------------|-------------------|
| **LSTM (este)** | 70.65% | 1.3M | < 1s |
| CNN-Text | ~72% | 2M | < 1s |
| BERT-base | ~93% | 110M | ~5s |
| RoBERTa | ~96% | 125M | ~6s |

**Conclusi√≥n:** El modelo LSTM ofrece un excelente balance entre rendimiento y eficiencia para aplicaciones con recursos limitados.

---

## Trabajo Futuro

Posibles mejoras y extensiones:

1. **Embeddings pre-entrenados**
   - Usar GloVe o Word2Vec
   - Transfer learning con embeddings contextuales

2. **Arquitecturas avanzadas**
   - Bi-directional LSTM
   - LSTM con mecanismos de atenci√≥n
   - Comparar con Transformers (BERT, RoBERTa)

3. **An√°lisis de errores**
   - Estudiar casos donde el modelo falla
   - Identificar patrones ling√º√≠sticos problem√°ticos

4. **Optimizaci√≥n**
   - Grid search de hiperpar√°metros
   - Pruning del modelo
   - Quantizaci√≥n para edge devices

5. **Ensemble de modelos**
   - Combinar LSTM + CNN
   - Voting classifier con m√∫ltiples arquitecturas

---

## üìö Referencias

- Maas, A. L., et al. (2011). *Learning word vectors for sentiment analysis*. ACL 2011.
- Hochreiter, S., & Schmidhuber, J. (1997). *Long short-term memory*. Neural Computation, 9(8), 1735-1780.
- Paszke, A., et al. (2019). *PyTorch: An imperative style, high-performance deep learning library*. NeurIPS 2019.
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.

---

## üìÑ Licencia

Este proyecto es un trabajo acad√©mico para el curso de Redes Neuronales de la Facultad de Ciencias, UNAM.

---


<div align="center">

** Si este proyecto te fue √∫til, considera darle una estrella **

Hecho con ‚ù§Ô∏è para el curso de Redes Neuronales - FC UNAM

</div>
